{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec38f19-196b-411c-9a99-909e3e4cd205",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ce8bc9-47ea-41da-85b9-0229d9021251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union, Optional, Sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from anndata import AnnData\n",
    "from numpy.random import default_rng\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial.distance import pdist\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "from scanpy import logging as logg\n",
    "import squidpy as sq\n",
    "import logging\n",
    "import psutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "from xen5k_utils import *\n",
    "\n",
    "hdir = '/home/workspace/'\n",
    "srldir = hdir + 'xen_5k_ibd_ra/objects/'\n",
    "\n",
    "adatas = load(srldir + 'processed_adatas.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2405fa-316a-4028-acb3-114a58c596e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(output_dir: str, name: str = 'ripley_analysis') -> logging.Logger:\n",
    "    \"\"\"Set up logger with file and console handlers.\"\"\"\n",
    "    # Create logs directory if it doesn't exist\n",
    "    log_dir = os.path.join(output_dir, 'logs')\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Create logger\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Create formatters\n",
    "    file_formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - Memory: %(memory).2f MB - %(message)s'\n",
    "    )\n",
    "    console_formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # File handler\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    fh = logging.FileHandler(\n",
    "        os.path.join(log_dir, f'ripley_analysis_{timestamp}.log')\n",
    "    )\n",
    "    fh.setLevel(logging.INFO)\n",
    "    fh.setFormatter(file_formatter)\n",
    "    \n",
    "    # Console handler\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    ch.setFormatter(console_formatter)\n",
    "    \n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c003036-f78b-45f7-97f6-a9f6467df818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e534b-3e71-4d03-8df5-cd0feaf2ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryLogFilter(logging.Filter):\n",
    "    \"\"\"Add memory usage to log record.\"\"\"\n",
    "    def filter(self, record):\n",
    "        record.memory = get_memory_usage()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7272d7d4-6faa-4f5a-aedc-5fc25e5e1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _worker_analyze_gene(args: tuple) -> dict:\n",
    "    gene_data, shared_data = args\n",
    "    gene_name, gene_expr = gene_data\n",
    "    \n",
    "    try:\n",
    "        # Unpack shared data\n",
    "        coordinates = shared_data['coordinates']\n",
    "        ref_high = shared_data['ref_high']\n",
    "        support = shared_data['support']\n",
    "        null_distributions = shared_data['null_distributions']\n",
    "        expression_threshold = shared_data['expression_threshold']\n",
    "        min_cells = shared_data['min_cells']\n",
    "        N = shared_data['N']\n",
    "        area = shared_data['area']\n",
    "        \n",
    "        # Process gene\n",
    "        test_high = gene_expr > np.quantile(gene_expr, expression_threshold)\n",
    "        joint_coords = coordinates[ref_high & test_high]\n",
    "        \n",
    "        if joint_coords.shape[0] < min_cells:\n",
    "            return None\n",
    "            \n",
    "        # Calculate L function\n",
    "        distances = pdist(joint_coords)\n",
    "        n_pairs_less_than_d = (distances < support.reshape(-1, 1)).sum(axis=1)\n",
    "        intensity = N / area\n",
    "        k_estimate = ((n_pairs_less_than_d * 2) / N) / intensity\n",
    "        l_stat = np.sqrt(k_estimate / np.pi)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        max_deviation = np.max(np.abs(l_stat - np.mean(null_distributions, axis=0)))\n",
    "        pvalues = np.array([\n",
    "            (np.sum(null_distributions[:, d] >= l_stat[d]) + 1) / (len(null_distributions) + 1)\n",
    "            for d in range(len(support))\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'gene': gene_name,\n",
    "            'l_stat': l_stat,\n",
    "            'max_deviation': max_deviation,\n",
    "            'min_pvalue': np.min(pvalues),\n",
    "            'pvalues': pvalues\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'gene': gene_name,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b134a-65a1-43a4-a567-6dcc8466fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_ripley_test(\n",
    "    adata: AnnData,\n",
    "    reference_gene: str,\n",
    "    output_dir: str,\n",
    "    spatial_key: str = 'spatial',\n",
    "    expression_threshold: float = 0,\n",
    "    n_simulations: int = 100,\n",
    "    max_dist: Optional[float] = None,\n",
    "    n_steps: int = 50,\n",
    "    min_cells: int = 10,\n",
    "    seed: Optional[int] = None,\n",
    "    n_jobs: Optional[int] = None\n",
    ") -> None:\n",
    "\n",
    "    # Setup logging\n",
    "    logger = setup_logger(output_dir)\n",
    "    logger.addFilter(MemoryLogFilter())\n",
    "    \n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Starting Ripley's L analysis for reference gene: {reference_gene}\")\n",
    "    logger.info(f\"Initial dataset shape: {adata.shape}\")\n",
    "    \n",
    "    try:\n",
    "        # Get spatial coordinates\n",
    "        coordinates = adata.obsm[spatial_key]\n",
    "        logger.info(f\"Loaded spatial coordinates with shape: {coordinates.shape}\")\n",
    "        \n",
    "        # Get reference gene expression\n",
    "        ref_expr = adata[:, reference_gene].X.toarray().flatten()\n",
    "        ref_high = ref_expr > np.quantile(ref_expr, expression_threshold)\n",
    "        logger.info(f\"Reference gene high expression cells: {np.sum(ref_high)}\")\n",
    "        \n",
    "        # Prepare support\n",
    "        hull = ConvexHull(coordinates)\n",
    "        area = hull.volume\n",
    "        if max_dist is None:\n",
    "            max_dist = (area / 2) ** 0.5\n",
    "        support = np.linspace(0, max_dist, n_steps)\n",
    "        logger.info(f\"Prepared support with max_dist: {max_dist:.2f}\")\n",
    "        \n",
    "        # Compute null model\n",
    "        logger.info(\"Starting null model computation...\")\n",
    "        null_start = time.time()\n",
    "        rng = default_rng(seed)\n",
    "        null_distributions = np.empty((n_simulations, len(support)))\n",
    "        \n",
    "        for i in range(n_simulations):\n",
    "            if i % 10 == 0:\n",
    "                logger.info(f\"Computing null distribution {i}/{n_simulations}\")\n",
    "            n_points = max(min_cells, int(np.sum(ref_high) * 0.1))\n",
    "            random_mask = rng.random(coordinates.shape[0]) < (n_points / coordinates.shape[0])\n",
    "            random_coords = coordinates[random_mask]\n",
    "            \n",
    "            distances = pdist(random_coords)\n",
    "            n_pairs = (distances < support.reshape(-1, 1)).sum(axis=1)\n",
    "            intensity = coordinates.shape[0] / area\n",
    "            k_est = ((n_pairs * 2) / coordinates.shape[0]) / intensity\n",
    "            null_distributions[i] = np.sqrt(k_est / np.pi)\n",
    "        \n",
    "        null_time = time.time() - null_start\n",
    "        logger.info(f\"Completed null model computation in {null_time:.2f} seconds\")\n",
    "        \n",
    "        # Prepare shared data\n",
    "        shared_data = {\n",
    "            'coordinates': coordinates,\n",
    "            'ref_high': ref_high,\n",
    "            'support': support,\n",
    "            'null_distributions': null_distributions,\n",
    "            'expression_threshold': expression_threshold,\n",
    "            'min_cells': min_cells,\n",
    "            'N': coordinates.shape[0],\n",
    "            'area': area\n",
    "        }\n",
    "        \n",
    "        # Prepare gene data\n",
    "        logger.info(\"Starting gene data preparation...\")\n",
    "        gene_prep_start = time.time()\n",
    "        total_genes = len(adata.var_names) - 1  # excluding reference gene\n",
    "        \n",
    "        gene_data = []\n",
    "        for i, gene in enumerate(tqdm(adata.var_names, desc=\"Preparing genes\")):\n",
    "            if gene == reference_gene:\n",
    "                continue\n",
    "            \n",
    "            if i % 100 == 0:  # Log progress every 100 genes\n",
    "                current_memory = get_memory_usage()\n",
    "                logger.info(f\"Processing gene {i}/{total_genes} ({gene}). Memory usage: {current_memory/1024:.2f} GB\")\n",
    "            \n",
    "            gene_data.append((gene, adata[:, gene].X.toarray().flatten()))\n",
    "        \n",
    "        gene_prep_time = time.time() - gene_prep_start\n",
    "        logger.info(f\"Completed gene data preparation in {gene_prep_time:.2f} seconds\")\n",
    "        logger.info(f\"Prepared analysis for {len(gene_data)} genes\")\n",
    "        \n",
    "        # Run parallel analysis\n",
    "        if n_jobs is None:\n",
    "            n_jobs = mp.cpu_count() - 1\n",
    "            \n",
    "        # Log memory state before parallel processing\n",
    "        current_memory = get_memory_usage()\n",
    "        logger.info(f\"Memory before parallel processing: {current_memory/1024:.2f} GB\")\n",
    "        logger.info(f\"Starting parallel analysis using {n_jobs} processes\")\n",
    "        \n",
    "        # Log size of shared data\n",
    "        shared_data_size = sum(x.nbytes if hasattr(x, 'nbytes') else 0 \n",
    "                             for x in shared_data.values() if hasattr(x, '__len__'))\n",
    "        logger.info(f\"Shared data size: {shared_data_size/1024/1024/1024:.2f} GB\")\n",
    "        \n",
    "        # Prepare arguments list with memory logging\n",
    "        logger.info(\"Preparing argument list for parallel processing...\")\n",
    "        args_start_memory = get_memory_usage()\n",
    "        args = [(g, shared_data) for g in gene_data]\n",
    "        args_end_memory = get_memory_usage()\n",
    "        logger.info(f\"Arguments list preparation changed memory usage from {args_start_memory/1024:.2f} GB to {args_end_memory/1024:.2f} GB\")\n",
    "        \n",
    "        parallel_start = time.time()\n",
    "        try:\n",
    "            with mp.Pool(n_jobs) as pool:\n",
    "                worker = partial(_worker_analyze_gene)\n",
    "                \n",
    "                # Create a monitoring function to wrap tqdm\n",
    "                def monitor_progress(iterator):\n",
    "                    for i, item in enumerate(iterator):\n",
    "                        if i % 100 == 0:  # Log every 100 genes\n",
    "                            current_memory = get_memory_usage()\n",
    "                            logger.info(f\"Processed {i} genes. Current memory: {current_memory/1024:.2f} GB\")\n",
    "                        yield item\n",
    "                \n",
    "                # Run the parallel processing with monitoring\n",
    "                results_list = list(tqdm(\n",
    "                    monitor_progress(pool.imap(worker, args)),\n",
    "                    total=len(args),\n",
    "                    desc=\"Processing genes\"\n",
    "                ))\n",
    "                \n",
    "                # Log final memory usage\n",
    "                final_memory = get_memory_usage()\n",
    "                logger.info(f\"Final memory after parallel processing: {final_memory/1024:.2f} GB\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_memory = get_memory_usage()\n",
    "            logger.error(f\"Parallel processing failed. Memory at error: {error_memory/1024:.2f} GB\")\n",
    "            logger.error(f\"Error during parallel processing: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "        \n",
    "        parallel_time = time.time() - parallel_start\n",
    "        logger.info(f\"Completed parallel analysis in {parallel_time:.2f} seconds\")\n",
    "        \n",
    "        # Process results\n",
    "        results_list = [r for r in results_list if r is not None]\n",
    "        errors = [r for r in results_list if 'error' in r]\n",
    "        if errors:\n",
    "            logger.warning(f\"Encountered errors in {len(errors)} genes\")\n",
    "            for error in errors[:5]:  # Log first 5 errors\n",
    "                logger.warning(f\"Error in gene {error['gene']}: {error['error']}\")\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        valid_results = [r for r in results_list if 'error' not in r]\n",
    "        df = pd.DataFrame([{\n",
    "            'gene': r['gene'],\n",
    "            'max_deviation': r['max_deviation'],\n",
    "            'min_pvalue': r['min_pvalue']\n",
    "        } for r in valid_results])\n",
    "        \n",
    "        # Sort by max_deviation\n",
    "        df = df.sort_values('max_deviation', ascending=False)\n",
    "        \n",
    "        # Save results\n",
    "        results_path = os.path.join(output_dir, \"ripley_results.csv\")\n",
    "        df.to_csv(results_path, index=False)\n",
    "        logger.info(f\"Saved results to {results_path}\")\n",
    "        \n",
    "        # Plot top genes\n",
    "        logger.info(\"Generating plots for top genes...\")\n",
    "        top_genes = ['CXCL9'] + df.nlargest(5, 'max_deviation')['gene'].tolist()\n",
    "        plot_path = os.path.join(output_dir, \"ripley_plots.pdf\")\n",
    "        sq.pl.spatial_scatter(\n",
    "            adata,\n",
    "            library_id=\"spatial\",\n",
    "            color=top_genes,\n",
    "            shape=None,\n",
    "            size=2,\n",
    "            save=plot_path\n",
    "        )\n",
    "        logger.info(f\"Saved plots to {plot_path}\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        logger.info(f\"Analysis completed in {total_time:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Analysis failed: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        # Clean up logging handlers\n",
    "        for handler in logger.handlers[:]:\n",
    "            handler.close()\n",
    "            logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728c5e7-3368-44da-9e46-3c8f36296f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_ripley_test(\n",
    "    sdatas['ibd'].tables[\"subsample\"],\n",
    "    reference_gene='GREM1',\n",
    "    output_dir=output_dir,\n",
    "    expression_threshold=0.7,\n",
    "    n_jobs=16,\n",
    "    n_steps=25\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xen5k_env)",
   "language": "python",
   "name": "xen5k_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
